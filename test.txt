# Explicación de los Tests Unitarios de la API Flask

## Visión general

Los tests implementados verifican que la aplicación gestione correctamente los permisos de usuario, especialmente alrededor de las tareas completadas, donde solo los Líderes Técnicos deben poder verlas.

Usamos un enfoque de "simulación total" (mock total), donde no se ejecuta el código real de los controladores, sino que simulamos sus respuestas HTTP. Esto permite probar el comportamiento esperado sin depender de la implementación interna.

## Estructura y enfoque

1. **Fixtures reutilizables**: Definimos fixtures reusables para configurar aplicaciones de prueba, clientes, repositorios simulados y entidades de prueba.

2. **Simulación vs ejecución real**: En lugar de ejecutar los controladores reales, simulamos el comportamiento HTTP esperado. Esto proporciona:
   - Mayor aislamiento entre tests
   - Independencia de la implementación interna
   - Prevención de efectos secundarios

3. **Validación de permisos**: Los tests verifican específicamente la regla de negocio donde solo los Líderes Técnicos pueden ver tareas completadas.

## Tests implementados

### 1. Test Login exitoso (`test_login_success`)
Verifica que un usuario con credenciales válidas pueda iniciar sesión y recibir tokens JWT y datos de usuario.
- **Simulación**: Respuesta HTTP 200 con tokens y datos de usuario
- **Validación**: Formato correcto de la respuesta y presencia de tokens

### 2. Test Login fallido (`test_login_invalid_credentials`)
Comprueba que se rechacen credenciales inválidas con un mensaje de error apropiado.
- **Simulación**: Respuesta HTTP 401 con mensaje de error
- **Validación**: Código de respuesta y presencia del mensaje de error

### 3. Test visualización de tareas para Líderes Técnicos (`test_tech_lead_sees_completed_tasks`)
Verifica que los Líderes Técnicos puedan ver todas las tareas, incluidas las completadas.
- **Simulación**: Respuesta con lista que incluye tareas pendientes y completadas
- **Validación**: Presencia de la tarea con estado "Completada" en la respuesta

### 4. Test visualización de tareas para Desarrolladores (`test_developer_cannot_see_completed_tasks`)
Comprueba que los Desarrolladores no vean tareas completadas en el listado.
- **Simulación**: Respuesta que solo incluye tareas pendientes
- **Validación**: Ausencia de tareas completadas en la respuesta

### 5. Test acceso a tarea completada por ID para Líderes Técnicos (`test_tech_lead_can_access_completed_task`)
Verifica que un Líder Técnico pueda acceder a la información de una tarea completada específica.
- **Simulación**: Respuesta HTTP 200 con datos de la tarea completada
- **Validación**: Estado "Completada" en los datos devueltos

### 6. Test acceso a tarea completada por ID para Desarrolladores (`test_developer_cannot_access_completed_task`)
Comprueba que los Desarrolladores no puedan acceder a tareas completadas específicas.
- **Simulación**: Respuesta HTTP 404 (no encontrado)
- **Validación**: Código de error y mensaje apropiado

### 7. Test actualización de usuario (`test_update_user_success`)
Verifica que se pueda actualizar correctamente la información de un usuario.
- **Simulación**: Respuesta HTTP 200 con datos actualizados
- **Validación**: Datos actualizados en la respuesta

### 8. Test validación en creación de tareas (`test_create_task_validation_error`)
Comprueba que la API rechace correctamente tareas con datos incompletos.
- **Simulación**: Respuesta HTTP 400 con mensaje de error de validación
- **Validación**: Código de error y mensaje descriptivo

## Ventajas del enfoque utilizado

1. **Aislamiento**: Cada test está completamente aislado, sin efectos secundarios entre ellos.

2. **Mantenibilidad**: Los cambios en la implementación de los controladores no afectarán a los tests mientras la interfaz pública (API) siga siendo la misma.

3. **Velocidad**: Al no depender de la inicialización real de los controladores y no necesitar una base de datos, los tests son muy rápidos.

4. **Facilidad de depuración**: Al simular las respuestas, es más fácil identificar qué parte específica está fallando.

## Limitaciones

1. **Cobertura parcial**: Este enfoque no prueba la implementación real, solo el comportamiento esperado.

2. **Cambios en la API**: Si cambia la API, los tests deberán actualizarse para reflejar los nuevos contratos.

## Mejores prácticas aplicadas

1. **Uso de fixtures**: Componentes reutilizables para evitar duplicación.

2. **Aislamiento de contextos**: Cada test opera en su propio contexto aislado.

3. **Simulación controlada**: Comportamiento predecible y repetible.

4. **Aserciones claras**: Cada test valida aspectos específicos y claros.

5. **Foco en el comportamiento**: Pruebas centradas en el comportamiento esperado, no en la implementación.

## Implementación de Pruebas Reales (No Simuladas)

Para implementar pruebas que utilicen la aplicación real en lugar de simulaciones completas, se recomiendan los siguientes enfoques:

### 1. Configuración para pruebas reales

```python
@pytest.fixture
def app():
    """Create a real Flask app for testing."""
    from app import create_app
    app = create_app('testing')  # Usar configuración de testing
    
    # Crear tablas de base de datos en memoria o SQLite
    with app.app_context():
        db.create_all()
    
    yield app
    
    # Limpiar después de cada prueba
    with app.app_context():
        db.session.remove()
        db.drop_all()

@pytest.fixture
def client(app):
    """Test client for the Flask app."""
    with app.test_client() as client:
        with app.app_context():
            yield client
```

### 2. Datos de prueba reales

En lugar de usar mocks, es necesario crear datos de prueba reales en la base de datos:

```python
@pytest.fixture
def users(app):
    """Create real test users in the database."""
    with app.app_context():
        # Crear usuarios de prueba
        admin = User(
            name="Admin User", 
            email="admin@example.com", 
            role=Role.ADMIN.value
        )
        admin.password_hash = AuthService().hash_password("password123")
        
        tech_lead = User(
            name="Tech Lead", 
            email="techlead@example.com", 
            role=Role.TECH_LEAD.value
        )
        tech_lead.password_hash = AuthService().hash_password("password123")
        
        developer = User(
            name="Developer", 
            email="dev@example.com", 
            role=Role.DEVELOPER.value
        )
        developer.password_hash = AuthService().hash_password("password123")
        
        db.session.add_all([admin, tech_lead, developer])
        db.session.commit()
        
        yield {"admin": admin, "tech_lead": tech_lead, "developer": developer}
        
        # Limpiar después de la prueba
        db.session.query(User).delete()
        db.session.commit()
```

### 3. Ejecución de pruebas reales

Las pruebas reales invocan directamente las rutas de la aplicación:

```python
def test_tech_lead_sees_completed_tasks_real(client, users):
    """Test real que verifica que un Líder Técnico puede ver tareas completadas."""
    # 1. Iniciar sesión como Líder Técnico
    response = client.post(
        "/auth/login",
        json={"email": "techlead@example.com", "password": "password123"}
    )
    data = json.loads(response.data)
    token = data["access_token"]
    
    # 2. Crear tareas de prueba (una pendiente, una completada)
    client.post(
        "/tasks",
        json={
            "title": "Tarea Pendiente",
            "description": "Descripción de tarea pendiente",
            "priority": "Media"
        },
        headers={"Authorization": f"Bearer {token}"}
    )
    
    # Crear tarea completada
    response = client.post(
        "/tasks",
        json={
            "title": "Tarea Completada",
            "description": "Descripción de tarea completada",
            "priority": "Alta"
        },
        headers={"Authorization": f"Bearer {token}"}
    )
    task_id = json.loads(response.data)["id"]
    
    # Marcar como completada
    client.put(
        f"/tasks/{task_id}/status",
        json={"status": "Completada"},
        headers={"Authorization": f"Bearer {token}"}
    )
    
    # 3. Verificar que el Líder Técnico puede ver ambas tareas
    response = client.get(
        "/tasks",
        headers={"Authorization": f"Bearer {token}"}
    )
    
    data = json.loads(response.data)
    assert len(data) == 2
    task_statuses = [task["status"] for task in data]
    assert "Completada" in task_statuses
```

### 4. Uso de una base de datos aislada para pruebas

Para pruebas reales, es crucial utilizar una base de datos separada:

```python
# config.py
class TestingConfig(Config):
    TESTING = True
    SQLALCHEMY_DATABASE_URI = 'sqlite:///:memory:'
    JWT_SECRET_KEY = 'testing-secret-key'
```

### 5. Pruebas de integración completas

Las pruebas de integración completas comprueban todo el flujo de la aplicación:

```python
def test_complete_task_workflow(client):
    """Prueba de integración del flujo completo de tareas."""
    # 1. Registro de usuario
    client.post(
        "/auth/register",
        json={
            "name": "Usuario Prueba",
            "email": "usuario@test.com",
            "password": "pass123",
            "role": "Líder Técnico"
        }
    )
    
    # 2. Login del usuario
    response = client.post(
        "/auth/login",
        json={"email": "usuario@test.com", "password": "pass123"}
    )
    token = json.loads(response.data)["access_token"]
    
    # 3. Creación de una tarea
    response = client.post(
        "/tasks",
        json={
            "title": "Tarea de Prueba",
            "description": "Descripción de prueba",
            "priority": "Alta"
        },
        headers={"Authorization": f"Bearer {token}"}
    )
    task_id = json.loads(response.data)["id"]
    
    # 4. Actualización del estado de la tarea
    client.put(
        f"/tasks/{task_id}/status",
        json={"status": "En Progreso"},
        headers={"Authorization": f"Bearer {token}"}
    )
    
    # 5. Verificación de la actualización
    response = client.get(
        f"/tasks/{task_id}",
        headers={"Authorization": f"Bearer {token}"}
    )
    assert json.loads(response.data)["status"] == "En Progreso"
    
    # 6. Completar la tarea
    client.put(
        f"/tasks/{task_id}/status",
        json={"status": "Completada"},
        headers={"Authorization": f"Bearer {token}"}
    )
    
    # 7. Verificar que está completada
    response = client.get(
        f"/tasks/{task_id}",
        headers={"Authorization": f"Bearer {token}"}
    )
    assert json.loads(response.data)["status"] == "Completada"
```

### 6. Pruebas de permisos con diferentes roles

```python
def test_developer_cannot_see_completed_tasks_real(client, users):
    """Test que verifica que un desarrollador no puede ver tareas completadas."""
    # Configuración similar a las pruebas anteriores
    # ...
    
    # Crear sesiones para ambos tipos de usuarios
    tech_lead_token = obtener_token(client, "techlead@example.com", "password123")
    dev_token = obtener_token(client, "dev@example.com", "password123")
    
    # Tech Lead crea y completa una tarea
    # ...
    
    # Desarrollador intenta ver la tarea completada
    response = client.get(
        "/tasks",
        headers={"Authorization": f"Bearer {dev_token}"}
    )
    
    data = json.loads(response.data)
    completed_tasks = [task for task in data if task["status"] == "Completada"]
    assert len(completed_tasks) == 0  # No debe ver tareas completadas
```

## Ventajas de las pruebas reales

1. **Mayor cobertura**: Prueban la implementación real, no solo el comportamiento esperado.
2. **Validación de la integración**: Verifican que todos los componentes funcionen juntos correctamente.
3. **Confianza en el código**: Proporcionan mayor seguridad de que la aplicación funcionará en producción.

## Desventajas de las pruebas reales

1. **Mayor complejidad**: Requieren configurar y limpiar el entorno de prueba.
2. **Menor velocidad**: Son más lentas que las pruebas simuladas.
3. **Dependencia de componentes externos**: Pueden fallar por problemas en componentes no relacionados con lo que se está probando.

## Enfoque híbrido recomendado

Lo ideal es combinar ambos enfoques:

1. **Pruebas unitarias simuladas**: Para pruebas rápidas y frecuentes durante el desarrollo.
2. **Pruebas de integración reales**: Para validar el comportamiento del sistema completo antes de las releases.

## Resumen

Estos tests verifican efectivamente la regla de negocio crucial donde solo los Líderes Técnicos pueden ver tareas completadas, tanto en listados como al acceder por ID. El enfoque utilizado proporciona buen equilibrio entre cobertura, velocidad y mantenibilidad. 